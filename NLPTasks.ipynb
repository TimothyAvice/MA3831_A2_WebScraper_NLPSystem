{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NLP Tasks\n",
    "## NLP Task 1 - Name Entity Recognition\n",
    "\n",
    "### Literature Review\n",
    "\n",
    "Name entity recognition has been used in a variety of papers for the purposes of brand name recognition. Within the\n",
    "Electronic commerce research journal an article was written that investigated name entity recognition of cross-border\n",
    "e-commerce titles based on TWs-LSTM. The TFIDF algorithm is used to manipulate the text corpus of the commodity to\n",
    "extract a weighted matrix, meanwhile the word2vec model represented the semantic meaning of the words extracted from the\n",
    "corpus’s bag of words. Where combined into a one-dimensional matrix that was passed into LTSM for commodity name entity\n",
    "recognition. The final accuracy achieved by this model was approximately 65% which was much higher than other models at\n",
    "the time (Luo et al., 2019).\n",
    "\n",
    "### Rational\n",
    "\n",
    "The proposed solution to the overall problem is to create an outlook system based on brands. Each review that was\n",
    "extracted utilising the webscraper had the model of the laptop along with the brand that it is made by. Name Entity\n",
    "recognition can be used to go through the review title for each review and extract this brand based on its\n",
    "classification. To accomplish this different entity name recognition systems can be utilised, or a model can be created\n",
    "and trained specifically for the desired purpose.\n",
    "\n",
    "Both the spacey and Stanford NER models were used and compared, with the results from the one with the highest accuracy\n",
    "being appended to the dataset. Each of these have facilities to import custom labelled datasets and train the models\n",
    "based of them, which as will not be utilised as a labelled training set is not available, instead the pretrained models\n",
    "will be used.\n",
    "\n",
    "The Stanford model utilises the conditional random field (CRF) algorithm which is a combination of the Hidden markov\n",
    "model and the Maximum entropy markov model.  This algorithm assumes that features are dependent on each other which\n",
    "would be the case with a sentence as the context is created by the dependency of words onto each other (Dasagrandhi,\n",
    "2020). Conditional random field algorithm makes use of this contextual information from the previous labels to\n",
    "better the current prediction (Chawla, 2021).\n",
    "\n",
    "Spacey utilises an unpublished algorithm but it utilises a word embedding method in conjunction with a deep\n",
    "convolutional neural network. This provides a model that has high adaptability and well-rounded accuracy (Dasagrandhi,\n",
    "2020).\n",
    "\n",
    "### Pre-processing\n",
    "The review titles where able to be directly inputted into the models with no changes requires as they were only singular\n",
    "sentence inputs.\n",
    "\n",
    "### Preliminary analysis\n",
    "\n",
    "The performance of the model will be based on the number of detections made and the correctness of those detections.\n",
    "Spacey has a higher detection rate compared to the Stanford model, however most of the detections from spacey are wrong.\n",
    "It recognises certain words as companies because of the use of capitalisation such as the detection of “Laptop Review”\n",
    "as an ORG when it is clearly not. When the results of the Stanford NER model are assessed, it has a lower detection rate\n",
    "but all the detections that are performed are correct.\n",
    "\n",
    "![Figure 1 - NER Results](NERResults.PNG)\n",
    "\n",
    "It should be noted that the spacey model is significantly faster than the Stanford model with it taking only 7 seconds\n",
    "to process the text as opposed to Stanford’s 22 minutes.\n",
    "\n",
    "Comparing these results to the article assessed, it has the same level of performance with less complexity. The article\n",
    "is dated back to 2012 which in the period since, more sophisticated models have been developed with a much higher\n",
    "success rate.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Packages\n",
    "import os\n",
    "\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "java_path = r\"C:\\Program Files\\Java\\jre1.8.0_291\\bin\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "\n",
    "def spacy(data):\n",
    "    brandNames = []\n",
    "    nlp = en_core_web_sm.load()\n",
    "    for i in tqdm.tqdm(range(len(data['Title']))):\n",
    "        organizations = []\n",
    "        classified_text = nlp(data.loc[i, 'Title'])\n",
    "        for item in classified_text.ents:\n",
    "            if item.label_ == 'ORG':\n",
    "                organizations.append(item.text)\n",
    "        if len(organizations) == 0:\n",
    "            brandNames.append(\"No brand found\")\n",
    "        else:\n",
    "            brandNames.append(organizations[0])\n",
    "\n",
    "    counter = 0\n",
    "    for brand in brandNames:\n",
    "        if brand == \"No brand found\":\n",
    "            counter += 1\n",
    "\n",
    "    print(\"Number of non branded entries: \" + str(counter))\n",
    "    print(\"Percentage detection: \" + str(((len(brandNames) - counter)/(len(brandNames) + 0.0))*100))\n",
    "\n",
    "    return brandNames\n",
    "\n",
    "\n",
    "def namedEntityRecognition(data):\n",
    "    brandNames = []\n",
    "    model_filename = './stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "    path_to_jar = './stanford-ner-2020-11-17/stanford-ner.jar'\n",
    "    st = StanfordNERTagger(model_filename=model_filename, path_to_jar=path_to_jar, encoding='utf-8')\n",
    "    for i in tqdm.tqdm(range(len(data['Title']))):\n",
    "        organizations = []\n",
    "        tokens = word_tokenize(data.loc[i, 'Title'])\n",
    "        classified_text = st.tag(tokens)\n",
    "        for item in classified_text:\n",
    "            if item[1] == 'ORGANIZATION':\n",
    "                organizations.append(item[0])\n",
    "        if len(organizations) == 0:\n",
    "            brandNames.append(\"No brand found\")\n",
    "        else:\n",
    "            brandNames.append(organizations[0])\n",
    "\n",
    "    counter = 0\n",
    "    for brand in brandNames:\n",
    "        if brand == \"No brand found\":\n",
    "            counter += 1\n",
    "\n",
    "    print(\"Number of non branded entries: \" + str(counter))\n",
    "    print(\"Percentage detection: \" + str(((len(brandNames) - counter)/(len(brandNames) + 0.0))*100))\n",
    "\n",
    "    return brandNames\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv('output.csv')\n",
    "\n",
    "    brands = namedEntityRecognition(data)\n",
    "    brands2 = spacy(data)\n",
    "\n",
    "    print(brands)\n",
    "    print(brands2)\n",
    "\n",
    "    data['Brand'] = brands\n",
    "\n",
    "    data.to_csv('outputBrand.csv', index=False)\n",
    "\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLP Task 2 - Sentiment Analysis\n",
    "\n",
    "### Literature review\n",
    "Sentiment analysis is a natural language processing method that detects if a body of text expresses a positive, negative\n",
    "or neutral outlook by scoring the words within the text and performing a summation. An article was included in the\n",
    "applied soft computing journal in 2020 that analysed the sentiment of tourism reviews, proposing the technique of\n",
    "sentiment padding. Sentiment padding creates a more consistent sample size and improves the proportion of sentiment\n",
    "information within each review. The article goes onto use neural networks; more specifically deep learning sentiment\n",
    "analysis models named lexicon integrated two-channel CNN-LTSM family models. This model combines CNN and LTSM/BiLTSM\n",
    "branches in parallel in order to achieve more accurate results. Utilising this approach on a variety of complex\n",
    "datasets, it was found to outperform many baseline methods. The final accuracy achieved was 50.68% on a reversed SST\n",
    "dataset while 95% accuracy was obtained on a Chinese review dataset. This was 2% higher than a zero-padding method\n",
    "(Li et al., 2020).\n",
    "\n",
    "### Rational\n",
    "\n",
    "The sentiment of a body of text can be directly linked to the outlook of the laptop. A positive sentiment can be\n",
    "assessed as a positive outlook for the laptop which is inline with the proposed solution. For this purpose, the Vader\n",
    "model will be utilised which is a lexicon and rule-based sentiment analysis tool that is attuned to sentiments expressed\n",
    "in social media (Singh, 2021). As these reviews are based on opinions from a range of writers it should be well suited\n",
    "as social media sentiments are opinion based as well. Each section can be assigned a separate sentiment score which can\n",
    "then be assessed at a later stage based on what aspect of the laptop that the user is most\n",
    "interested in.\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "For this data as it was a large corpus some pre-processing was required. The first was the removal of any stop words\n",
    "from the text which added no additional meaning to the sentence such as joining words, this was done by tokenizing the\n",
    "words and removing any that appeared in NLTKs stop words list. Each segment of the data set besides the title was passed\n",
    "through this process (As the title was not used in this NLP task). Afterwards the non-text characters where removed such\n",
    "punctuation and brackets.\n",
    "\n",
    "Lastly the corpus for each segment was normalized using lemmatization which reduce words to their base form. This acts\n",
    "to reduce errors when analysing sentiment as there are less variations of each word allowing for a better sentiment\n",
    "score to be assigned at the end, the NLTK WordNetLemmatizer was used to perform this.\n",
    "\n",
    "### Preliminary Analysis\n",
    "\n",
    "The vader package was used to apply a sentiment analysis on the processed corpus for each segment which was appended to\n",
    "the dataset. This final dataset with all the sentiment for each of the section was saved into a csv document. To assess\n",
    "the performance of this process a few sections of text will be selected and their corresponding sentiment scores. It\n",
    "will then be manually reviewed to gauge if the mapped sentiment matches the actual overall outlook of the body of text.\n",
    "For this the verdict paragraph will be selected as this gives a finalised opinion of the laptop which will be the\n",
    "easiest to assess the outlook off.\n",
    "\n",
    "![Figure 2 - Verdict and corresponding sentiment score](SentimentResults.PNG)\n",
    "\n",
    "The sentiment analysis scores this verdict as highly positive with little to no negative sentiment detected. When the\n",
    "actual paragraph itself is assessed, there is only one sentence which has a negative outlook with the first half\n",
    "paragraph being highly positive. The compound rating is very high, much higher than what it should be. This could be a\n",
    "result of the pre-processing removing some meaning from the descriptive words in certain areas.\n",
    "\n",
    "When the overall sentiment of the verdict segment is assessed it appears to have a very large skew towards highly\n",
    "positive sentiment. This may be a resultant from the reviews being sorted from highest overall rating with the lowest\n",
    "rating scraped being a 74% (The data set is skewed more towards positive outlook).\n",
    "\n",
    "### Code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "def noiseRemoval(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i in tqdm.tqdm(range(data.shape[0])):\n",
    "        for j in range(data.iloc[0].shape[0] - 1):\n",
    "            tokens = word_tokenize(data.iloc[i, j + 1].lower())\n",
    "            if j != data.iloc[0].shape[0] - 1:\n",
    "                data.iloc[i, j + 1] = \" \".join([word for word in tokens if not word in stop_words])\n",
    "\n",
    "    data['Intro'] = [re.sub(r'\\W', ' ', i) for i in data['Intro']]\n",
    "    data['Case'] = [re.sub(r'\\W', ' ', i) for i in data['Case']]\n",
    "    data['Connectivity'] = [re.sub(r'\\W', ' ', i) for i in data['Connectivity']]\n",
    "    data['Input devices'] = [re.sub(r'\\W', ' ', i) for i in data['Input devices']]\n",
    "    data['Display'] = [re.sub(r'\\W', ' ', i) for i in data['Display']]\n",
    "    data['Performance'] = [re.sub(r'\\W', ' ', i) for i in data['Performance']]\n",
    "    data['Emissions'] = [re.sub(r'\\W', ' ', i) for i in data['Emissions']]\n",
    "    data['Energy management'] = [re.sub(r'\\W', ' ', i) for i in data['Energy management']]\n",
    "    data['Verdict'] = [re.sub(r'\\W', ' ', i) for i in data['Verdict']]\n",
    "\n",
    "    normalization(data)\n",
    "\n",
    "\n",
    "def normalization(data):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    for i in tqdm.tqdm(range(data.shape[0])):\n",
    "        for j in range(data.iloc[0].shape[0] - 1):\n",
    "            word_list = word_tokenize(data.iloc[i, j + 1])\n",
    "            data.iloc[i, j + 1] = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "\n",
    "    sentimentAnalysis(data)\n",
    "\n",
    "\n",
    "# Return sentiment of sentence. Returning 0.0 in all fields if it is none\n",
    "def vader(sentence):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    if sentence == \"none\":\n",
    "        return {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "    else:\n",
    "        return analyser.polarity_scores(sentence)\n",
    "\n",
    "\n",
    "def sentimentAnalysis(data):\n",
    "    segments = [[], [], [], [], [], [], [], [], []]\n",
    "    for index, row in tqdm.tqdm(data.iterrows()):\n",
    "        segments[0].append(vader(row['Intro']))\n",
    "        segments[1].append(vader(row['Case']))\n",
    "        segments[2].append(vader(row['Connectivity']))\n",
    "        segments[3].append(vader(row['Input devices']))\n",
    "        segments[4].append(vader(row['Display']))\n",
    "        segments[5].append(vader(row['Performance']))\n",
    "        segments[6].append(vader(row['Emissions']))\n",
    "        segments[7].append(vader(row['Energy management']))\n",
    "        segments[8].append(vader(row['Verdict']))\n",
    "\n",
    "    data['Intro sentiment'] = segments[0]\n",
    "    data['Case sentiment'] = segments[1]\n",
    "    data['Connectivity sentiment'] = segments[2]\n",
    "    data['Input devices sentiment'] = segments[3]\n",
    "    data['Display sentiment'] = segments[4]\n",
    "    data['Performance sentiment'] = segments[5]\n",
    "    data['Emissions sentiment'] = segments[6]\n",
    "    data['Energy management sentiment'] = segments[7]\n",
    "    data['Verdict sentiment'] = segments[8]\n",
    "\n",
    "    data.to_csv('outputBrandSentimentNoPreprocessing.csv', index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv('outputBrand.csv')\n",
    "    noiseRemoval(data)\n",
    "\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "Luo, Y., Ma, J., & Li, C. (2019). Entity name recognition of cross-border e-commerce commodity titles based on TWs-LSTM.\n",
    "Electronic Commerce Research, 20(2), 405–426. https://doi.org/10.1007/s10660-019-09371-6\n",
    "\n",
    "Dasagrandhi, C. S. (2020). Understanding Named Entity Recognition Pre-Trained Models. V-Soft Consulting.\n",
    "https://blog.vsoftconsulting.com/blog/understanding-named-entity-recognition-pre-trained-models#:%7E:text=Model%20Architecture,while%20learning%20a%20new%20pattern.&text=In%20terms%20of%20performance%2C%20it,methods%20for%20entity%20recognition%20problems.\n",
    "\n",
    "Chawla, R. (2021, April 19). Overview of Conditional Random Fields - ML 2 Vec. Medium.\n",
    "https://medium.com/ml2vec/overview-of-conditional-random-fields-68a2a20fa541\n",
    "\n",
    "Li, W., Zhu, L., Shi, Y., Guo, K., & Cambria, E. (2020). User reviews: Sentiment analysis using lexicon integrated\n",
    "two-channel CNN–LSTM​ family models. Applied Soft Computing, 94, 106435. https://doi.org/10.1016/j.asoc.2020.106435\n",
    "\n",
    "Singh, F. (2021, February 2). Sentiment Analysis Made Easy Using VADER. Analytics India Magazine.\n",
    "https://analyticsindiamag.com/sentiment-analysis-made-easy-using-vader/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}