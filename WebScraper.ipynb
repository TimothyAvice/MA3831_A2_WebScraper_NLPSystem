{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# WebScraper\n",
    "\n",
    "## Websites\n",
    "### Notebook Check\n",
    "\n",
    "Notebook check is a website orientated at providing performance benchmarks, comparisons, reviews, buyers guides and more\n",
    "for a variety of tech related products. It specifies that all writers who provide articles for the site are independent\n",
    "writers, indicating a decreased bias in the pieces written. This site was used as the prime data source because it has\n",
    "available a very large range of laptop articles/reviews, which is the focus of this analysis, when compared to other\n",
    "similar sites. Each review follows a similar layout making it simple to segment the different sections of the reviews.\n",
    "When a couple of reviews were extracted and observed, the following structure was found:\n",
    "\n",
    "•\tTitle\n",
    "\n",
    "•\tIntroduction\n",
    "\n",
    "•\tCase\n",
    "\n",
    "•\tConnectivity\n",
    "\n",
    "•\tInput devices\n",
    "\n",
    "•\tDisplay\n",
    "\n",
    "•\tPerformance\n",
    "\n",
    "•\tEmissions\n",
    "\n",
    "•\tEnergy management\n",
    "\n",
    "•\tVerdict\n",
    "\n",
    "Each of these can be viewed as key aspects that a consumer would be interested in when buying a laptop, making it easier\n",
    "to perform the NLP tasks later. On these pages there is also a section which provides a brief overlay of the key\n",
    "statistics; however, these will be gathered from the second website which provides a far more detailed specification\n",
    "breakdown.\n",
    "\n",
    "### Laptopmedia.com\n",
    "\n",
    "Laptopmedia is a site that has a range reviews, specifications and analysis for a very large range of laptops. The\n",
    "largest of these being the sheer number of specifications available for different devices. A reason this website was not\n",
    "used for the main review gathering section is that it only has reviews available for a very limited number of the\n",
    "laptops on the site while also making heavy use of javascript making it hard to navigate and extract the content on\n",
    "certain pages. Each specification was gathered by going to the laptop series page which has a complete list of all the\n",
    "sites laptops. This made it easy to go into the page for each laptop and extract the specification information.\n",
    "\n",
    "The specification was presented in a table format with labels, making it easy to locate where the data was and create a\n",
    "title for the entry. Hyperlinks where also provided for certain aspects which were also extracted for a more complete\n",
    "data set.\n",
    "\n",
    "## Copyright\n",
    "\n",
    "As the information extracted from the website is being used for personal use there should be no issues regarding the\n",
    "copyright. It is also not being extracted with the goal of achieving any form of financial gain in which case the\n",
    "authors and site owner themselves would need to be contacted, and some sort of written permission given to use the\n",
    "contents within. This would also need to be performed if this program were to be made publicly available, or the\n",
    "contents of this analysis be used in a published academic paper.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "To extract the webpage html for each of the pages of the website a combination of both selenium and beautiful soup was\n",
    "used. Selenium makes use of a webdriver that operates the page the same way in which a user would, it opens the webpage\n",
    "specified natively in the specified browser [1]. The browser selected was chrome, given it has the highest degree of\n",
    "familiarity (This required a separate installation and path specification). This allows for information to be extracted\n",
    "from sites where the contents are only populated when the page is opened (Javascript based sites). Selenium was only\n",
    "utilised to get a complete page html where the information can be extracted from using beautiful soup. Beautiful soup is\n",
    "a python library for extracting information from HTML and XML files [2].\n",
    "\n",
    "The webcrawler works by loading the main search page url into the webdriver which extracts the page html. Beautiful soup\n",
    "takes in this html and the a tag information is extracted for a specific class, being determined by manually looking\n",
    "through the structure of the page and where each element is contained. A list is created of all the laptop links doing\n",
    "this which is then looped through the webdriver to obtain the actual reviews themselves.\n",
    "\n",
    "For each of the laptop links a similar process was followed where the page html was extracted and then analysed based of\n",
    "the tags and class identification. In this case the information was contained within div tags. The title was easy to\n",
    "extract as it was in an isolated div tag with a specific class along with the intro/overview of the review. Another\n",
    "search was performed which extracted all the div tags that had a specific class which was the same for all text\n",
    "paragraphs in the review. This information was segmented into the different review areas based on the h2 tag contents\n",
    "(Which is the heading style used for the section titles). Some string formatting was performed and then this was\n",
    "appended to the laptops list.\n",
    "\n",
    "Once all the information for all the laptops had been extracted, it was then written to a csv using the base csv writer\n",
    "in python. For this whole process the tqdm package was used which allows for the progress of a loop to be viewed and it\n",
    "also gives an elapsed time and estimated time of completion. The same process was followed for laptop media with the\n",
    "only difference being the class used and the depth gone in the html.\n",
    "\n",
    "## Data Extraction\n",
    "\n",
    "When the HTML was analysed which contained the text it always followed the same structure where all the text of interest\n",
    "was contained within a <div> with the “content” id. Each segment of text was separated into a different div with a\n",
    "class of “ttcl_number csc-default”, the number in that class representing what type of content is in that segment. 1\n",
    "denoted the review/article main caption while 0 represented the content of interest. So to get the content of importance\n",
    "a find_all() method was used on the soup for div tags that had the class “ttcl_0 csc-default”. This resulted in a list\n",
    "of all the div tags and the content which could be further organised and analysed to extract the text and organise\n",
    "them. For each of these tags a sub soup was created which simple find() methods could be run on to extract the text\n",
    "contents and then append these to a list based on if they had a heading or not (If a h2 tag existed).\n",
    "\n",
    "The contents of these heading tags were compared to a range of words that where created based on initial site html\n",
    "inspection. An index was assigned based on this heading which specified what location in the laptop list it would get\n",
    "appended to. This laptop list was organised based on the section headings to make it easier to determine what data was\n",
    "where and making the final saving of the data to the csv easier. If no h2 tag was found in the div, the contained\n",
    "text would be a continuation of the current section and the index would not be changed.\n",
    "\n",
    "The raw data was able to be extracted from a div tag within with the class “csv-textpic-text” setting text=True in the\n",
    "find() method. Some initial formatting had to be performed on the text blocks such as the removal of any new line\n",
    "characters and making sure no double spaces where present. Also, if no text content was found for a certain heading\n",
    "(That’s if only an image is located under the heading, but the heading still exists), the content would just be set to\n",
    "the current heading.\n",
    "\n",
    "## Final Data\n",
    "\n",
    "Once this whole process was done the contents of the laptop array was written to a csv file. The final data set was\n",
    "comprised of 750 entries with 11 columns. One of the columns labelled “Specifications” would be substituted with data\n",
    "from the second crawler which gathered only specification information. All fields contained textual data organised by\n",
    "the sections in the reviews in which they came from such as; “Display”, “Performance”, “Case” etc.\n",
    "\n",
    "![Figure 1 - Final dataset](DataTableFigure.png)\n",
    "\n",
    "Not as large of a data set was able to be created as was desired because of the use of only one website, however on a\n",
    "finalised project this scraper would be run on multiple review sites.\n",
    "\n",
    "## Running\n",
    "\n",
    "In order to run this webcrawler the following packages are required\n",
    "\n",
    "•\tbs4\n",
    "\n",
    "•\tselenium\n",
    "\n",
    "•\ttqdm (To View the progress)\n",
    "\n",
    "It also requires for the ChromeDriver to be downloaded and added to the working directory for the user’s current\n",
    "version of chrome.\n",
    "\n",
    "![Figure 2 - Webscraper working](WebScraperWorking.PNG)\n",
    "\n",
    "The scraper opens the physical web page for each of the laptops and then scrapes the data. As can be seen in the\n",
    "progress bar it has scraped 3 out of 786 laptops and has an estimated time of 1 hour and 31 minutes remaining.\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from selenium import webdriver\n",
    "import tqdm\n",
    "\n",
    "# Chromedriver version 90 required to be installed\n",
    "\n",
    "def main():\n",
    "    # Get all the link for each laptop\n",
    "    website_url = 'https://www.notebookcheck.net/Reviews.55.0.html?&items_per_page=500&hide_youtube=1' \\\n",
    "                  '&ns_show_num_normal=1&hide_external_reviews=1&introa_search_title=laptop%20review&tagArray[' \\\n",
    "                  ']=16&typeArray[]=1 '\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    driver.get(website_url)\n",
    "\n",
    "    page_html = driver.page_source\n",
    "\n",
    "    page_soup = soup(page_html, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for a in page_soup.findAll(\"a\", {\"class\": \"introa_large introa_review\"}, href=True):\n",
    "        links.append(a['href'])\n",
    "    for a in page_soup.findAll(\"a\", {\"class\": \"introa_small introa_review\"}, href=True):\n",
    "        links.append(a['href'])\n",
    "\n",
    "    website_url = \"https://www.notebookcheck.net/Reviews.55.0.html?&items_per_page=500&hide_youtube=1\" \\\n",
    "                  \"&ns_show_num_normal=1&hide_external_reviews=1&page=1&introa_search_title=laptop%20review&tagArray[\" \\\n",
    "                  \"]=16&typeArray[]=1 \"\n",
    "\n",
    "    driver.get(website_url)\n",
    "\n",
    "    page_html = driver.page_source\n",
    "\n",
    "    page_soup = soup(page_html, 'html.parser')\n",
    "\n",
    "    for a in page_soup.findAll(\"a\", {\"class\": \"introa_large introa_review\"}, href=True):\n",
    "        links.append(a['href'])\n",
    "    for a in page_soup.findAll(\"a\", {\"class\": \"introa_small introa_review\"}, href=True):\n",
    "        links.append(a['href'])\n",
    "\n",
    "    labels = [\"Title\", \"Intro\", \"Specifications\", \"Case\", \"Connectivity\", \"Input devices\", \"Display\", \"Performance\",\n",
    "              \"Emissions\", \"Energy management\", \"Verdict\"]\n",
    "    laptops = [[], [], [], [], [], [], [], [], [], [], []]\n",
    "    # Make into for loop\n",
    "    for i in tqdm.tqdm(range(len(links))):\n",
    "        # for i in tqdm.tqdm(range(10)):\n",
    "        try:\n",
    "            driver.get(links[i])\n",
    "            page_html = driver.page_source\n",
    "            page_soup = soup(page_html, 'html.parser')\n",
    "            laptop = [[], [], [], [], [], [], [], [], [], [], []]\n",
    "            laptop[0].append(page_soup.find(\"h1\", text=True).text)\n",
    "            laptop[1].append(str(page_soup.find(\"div\", {\"class\": \"intro-text\"}).contents[1]).strip(\" \"))\n",
    "            temp = page_soup.find('div', {\"class\": \"csc-textpic-text\"}).findAll(text=True)\n",
    "\n",
    "            # Retrieve Intro information\n",
    "            for item in temp:\n",
    "                str(item).strip(\" \").replace('\\n', '')\n",
    "            temp = \" \".join(temp).replace(\"  \", \" \")\n",
    "            laptop[1][0] = laptop[1][0] + \" \" + temp\n",
    "\n",
    "            divs = page_soup.find_all(\"div\", {\"class\": \"ttcl_0 csc-default\"})\n",
    "            index = 0\n",
    "            heading = ''\n",
    "            for div in divs[1:]:\n",
    "                sub_soup = soup(str(div), 'html.parser')\n",
    "                if len(sub_soup.find_all(\"h2\")) > 0:\n",
    "                    heading = str(sub_soup.find(\"h2\", text=True).text).capitalize()\n",
    "                    if (\"Case\" in heading) or (\"Chassis\" in heading):\n",
    "                        index = 3\n",
    "                    elif (\"Connectivity\" in heading) or (\"Equipment\" in heading):\n",
    "                        index = 4\n",
    "                    elif \"Input\" in heading:\n",
    "                        index = 5\n",
    "                    elif \"Display\" in heading:\n",
    "                        index = 6\n",
    "                    elif \"Performance\" in heading:\n",
    "                        index = 7\n",
    "                    elif \"Emissions\" in heading:\n",
    "                        index = 8\n",
    "                    elif \"Energy\" in heading:\n",
    "                        index = 9\n",
    "                    elif \"Verdict\" in heading:\n",
    "                        index = 10\n",
    "\n",
    "                try:\n",
    "                    sub_temp = sub_soup.find('div', {\"class\": \"csc-textpic-text\"}).findAll(text=True)\n",
    "                    for item in sub_temp:\n",
    "                        str(item).strip(\" \").replace('\\n', '')\n",
    "\n",
    "                    laptop[index].append(str(\" \".join(sub_temp).replace(\"  \", \" \")))\n",
    "                    laptop[index][0].strip('\\n').replace(\"  \", \" \")\n",
    "\n",
    "                    if laptop[index][0] == '':\n",
    "                        laptop[index][0] = heading\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            for i in range(len(laptop)):\n",
    "                if len(laptop[i]) == 0:\n",
    "                    laptop[i].append(\"none\")\n",
    "                laptops[i].append(laptop[i])\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Saving to a csv file\n",
    "    with open('output.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(labels)\n",
    "        for i in tqdm.tqdm(range(len(laptops[0]))):\n",
    "            try:\n",
    "                laptop = []\n",
    "                for j in range(len(laptops)):\n",
    "                    laptop.append(laptops[j][i][0])\n",
    "                writer.writerow(laptop)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}